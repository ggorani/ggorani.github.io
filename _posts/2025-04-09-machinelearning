---
title:  "[ChatGPT와 함께하는 Machine Learning (1)]"
excerpt: "With ChatGPT"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]

toc: true
toc_sticky: true
 
date: 2025-04-09
last_modified_at: 2025-04-09
---


# 1. 🧠 머신러닝이란 무엇인가?
머신러닝은 데이터를 이용해 명시적 프로그래밍 없이 패턴을 학습하는 알고리즘이다.

사람이 “이 조건이면 이렇게 해!”라고 코드를 짜는 게 아니라, 컴퓨터가 데이터 보고 “아~ 이게 이렇구나~” 하면서 알아서 함수 형태를 찾아내는 거지.

📦 머신러닝
│
├── 1. 지도학습 (Supervised Learning)
│   ├── 입력(X) + 정답(Y) 제공
│   ├── 목적: 정답 예측
│   ├── 알고리즘 예시:
│   │   ├── 회귀: 선형 회귀 (Linear Regression)
│   │   └── 분류: 로지스틱 회귀, 결정트리, KNN, SVM, 신경망 등
│   └── 예: 이메일 → 스팸/아님, 집 면적 → 가격 예측
│
├── 2. 비지도학습 (Unsupervised Learning)
│   ├── 입력(X)만 존재 (정답 없음)
│   ├── 목적: 데이터 내 구조/패턴 탐색
│   ├── 알고리즘 예시:
│   │   ├── 군집화: K-평균, 계층 클러스터링
│   │   └── 차원 축소: PCA, t-SNE
│   └── 예: 고객 세그먼트 나누기, 이미지 압축
│
└── 3. 강화학습 (Reinforcement Learning)
    ├── 정답 없음, 대신 보상(Reward) 기반 학습
    ├── 목적: 최적의 행동(Policy) 학습
    ├── 핵심 요소: 상태(State), 행동(Action), 보상(Reward)
    └── 예: 게임 AI, 로봇 팔 제어, 추천 시스템




# 2. 📘 회귀 (Regression)

## 2-1. 💡 개념
- 연속적인 숫자 값을 예측하는 지도학습 알고리즘
- 예시:
  - 집 면적 → 집 가격
  - 광고비 → 매출
  - 공부 시간 → 시험 점수

## 2-2. 🔑 핵심 용어
- 입력값 (X): 예측의 기준이 되는 데이터
- 출력값 (Y): 우리가 예측하고 싶은 값
- 잔차 (Residual): 예측값과 실제값의 차이
- 선형 회귀 (Linear Regression): 데이터를 가장 잘 설명하는 직선을 찾는 것
- 과적합 (Overfitting): 훈련 데이터에 너무 맞춰져 일반화가 잘 안 되는 현상


## 2-3. 📘 회귀 (Regression) 정리

회귀는 숫자 값을 예측하는 지도학습(Supervised Learning)의 한 종류다.  
우리가 알고 싶은 연속적인 값을 입력 변수(X)를 통해 예측한다.


🌿 회귀 전체 흐름도

Regression (회귀)
├── 1. 선형 기반 (Linear Models)
│   ├── 1.1 단순 선형 회귀 (Simple Linear Regression)
│   ├── 1.2 다중 선형 회귀 (Multiple Linear Regression)
│   ├── 1.3 다항 회귀 (Polynomial Regression)
│   └── 1.4 정규화 회귀 (Regularized Linear Regression)
│       ├── 릿지 회귀 (Ridge)
│       ├── 라쏘 회귀 (Lasso)
│       └── 엘라스틱넷 (ElasticNet)
└── 2. 비선형 회귀 (Nonlinear Regression)
    └── 예: 지수 회귀, 로그 회귀, 시그모이드 회귀 등

---

### 2-3-1. 선형 기반 (Linear Models)

#### 2-3-1-1. 단순 선형회귀 (Simple Linear Regression)
- 하나의 입력 변수 X → 하나의 출력 Y를 예측
- 예: 공부 시간(X) → 시험 점수(Y)

#### 2-3-1-2. 다중 선형 회귀 (Multiple Linear Regression)
- 여러 개의 입력 변수 X₁, X₂, ... → 하나의 Y 예측
- 예: 집 면적, 층수, 방 개수 → 집 가격

> 위 두 개는 모두 직선(또는 평면)으로 데이터를 설명

---

#### 2-3-1-3. 다항 회귀 (Polynomial Regression)
- 입력 변수의 제곱, 세제곱 등을 포함해 곡선으로 예측
- 예: 광고 비용이 너무 많으면 오히려 효과가 줄어드는 경우

> 이름은 '비선형'처럼 보이지만, 여전히 **선형 회귀 계열**  
> 계수(가중치)는 선형으로 학습되기 때문

---

#### 2-3-1-4. 정규화 회귀 (Regularized Linear Regression)
- 목적: 과적합 방지, 변수 선택
- 선형 회귀 모델에 **페널티 항**을 더해 모델 복잡도를 조절

- 릿지 회귀 (Ridge Regression)
    - L2 정규화 적용
    - 모든 계수를 작게 만듦 (0에는 안 됨)
    - 수식: `Loss = MSE + α * Σ(W²)`

- 라쏘 회귀 (Lasso Regression)
    - L1 정규화 적용
    - 일부 계수를 0으로 만듦 → 변수 선택 기능 있음
    - 수식: `Loss = MSE + α * Σ(|W|)`

- 엘라스틱넷 (ElasticNet)
    - L1 + L2 정규화 혼합
    - 릿지의 안정성과 라쏘의 변수 선택 기능을 동시에

---

### 2-3-2. 비선형 회귀 (Nonlinear Regression)
- 비선형 함수로 입력과 출력의 관계를 표현
- 예:
  - 지수 회귀 (Exponential Regression)
  - 로그 회귀 (Logarithmic Regression)
  - 시그모이드 회귀 (Sigmoid Regression)

> 이들은 단순히 변수를 제곱하거나 정규화하는 걸 넘어서,  
> 아예 **비선형 함수 형태로 모델링**함

---

✅ 정리

- 대부분의 회귀는 **선형 모델**에서 출발해 확장됨
- 다항 회귀, 릿지, 라쏘, 엘라스틱넷은 모두 **선형 회귀 기반**
- 비선형 회귀는 데이터의 복잡한 형태를 직접 함수로 설명함

---

# Q1. 각 정규화 회귀 모델들은 어떤 상황에 적용하는가?
## A1. 정규화 회귀 모델 비교 (Ridge / Lasso / ElasticNet)

    정규화 회귀는 과적합을 방지하고, 모델의 일반화 성능을 높이기 위해  
    기존 선형 회귀에 **패널티(제약 조건)**를 추가한 방식이다.

    ---

    ## 🔹 Ridge Regression (릿지 회귀)

    - **정규화 방식**: L2
    - **작용**: 계수를 0에 가깝게 만듦 (완전히 0은 아님)
    - **적용 상황**:
    - 다중공선성이 존재할 때
    - 모든 변수의 영향력을 살리고 싶을 때
    - 전체적인 모델 안정성이 중요할 때
    - **예시**:
    - 여러 센서 데이터를 모두 고려해 예측 모델을 만들고 싶을 때

    ---

    ## 🔹 Lasso Regression (라쏘 회귀)

    - **정규화 방식**: L1
    - **작용**: 일부 계수를 **완전히 0**으로 만들어 변수 제거
    - **적용 상황**:
    - 불필요한 변수를 자동으로 제거하고 싶을 때
    - 변수 수가 너무 많을 때
    - 모델을 해석하기 쉽게 만들고 싶을 때
    - **예시**:
    - 유전자 분석, 마케팅 특성 추출 등 핵심 변수만 뽑고 싶을 때

    ---

    ## 🔹 ElasticNet (엘라스틱넷)

    - **정규화 방식**: L1 + L2 혼합
    - **작용**: Lasso의 변수 선택 + Ridge의 안정성 결합
    - **적용 상황**:
    - 변수 수가 많고, 서로 상관관계도 높은 경우
    - Lasso 단독 사용 시 모델 성능이 떨어질 때
    - Ridge도 Lasso도 애매하게 느껴질 때 → 절충형
    - **예시**:
    - 고차원 금융 데이터, 텍스트 특성 등 복잡한 변수 처리할 때

    ---

    ## ✅ 한 줄 요약!

    - **Ridge** → 모든 변수를 살리고 싶을 때
    - **Lasso** → 변수 선택하고 싶을 때
    - **ElasticNet** → 둘 사이에서 균형 잡고 싶을 때


# Q2. 다중공선성이란?
## A2. 다중공선성 (Multicollinearity)

    ## ✅ 정의

    다중공선성이란 **입력 변수(설명 변수)들 사이에 높은 상관관계**가 있는 상태를 말한다.  
    즉, 비슷한 정보를 가진 변수가 여럿 존재하면, 회귀 모델은 어떤 변수의 영향을 받아야 할지 **혼란스러운 상태**가 된다.

    ---

    ## 🎯 예시

    예를 들어 어떤 회귀 모델에 다음과 같은 변수가 있다고 해보자:

    - `X1`: 공부 시간  
    - `X2`: 푼 문제 수  
    - `Y`: 시험 점수

    공부 시간이 많을수록 푼 문제 수도 많아질 가능성이 크다.  
    즉, `X1`과 `X2`는 **서로 강한 상관관계**를 가진다.  
    이 경우 모델은 두 변수 중 어떤 것을 기준으로 예측해야 할지 **혼란스러워지고**,  
    회귀 계수가 **불안정하게 튈 수 있다**.

    ---

    ## 🧨 왜 문제가 되는가?

    - **회귀 계수가 불안정해진다**  
    → 데이터에 조금만 변화가 생겨도 계수가 크게 요동침

    - **모델 해석이 어려워진다**  
    → 어떤 변수가 정말 중요한지 판단이 모호해짐

    - **예측 성능은 괜찮아 보여도 일반화는 안 됨**  
    → 훈련 데이터에서는 잘 맞지만, 새로운 데이터에는 성능이 저하될 수 있음

    ---

    ## 🔍 다중공선성 탐지 방법

    - **상관계수 확인 (`corr()`)**
    - 변수들 간의 상관관계를 확인
    - 상관계수가 0.8 이상이면 다중공선성을 의심할 수 있음

    - **VIF (Variance Inflation Factor)**
    - 하나의 변수를 나머지 변수들로 회귀했을 때 설명되는 정도
    - 일반 기준:
        - VIF > 5 : 다중공선성 주의
        - VIF > 10 : 매우 강한 다중공선성

    ---

    ## 💡 해결 방법

    - **유사한 변수 제거**
    - 상관관계가 높은 변수들 중 하나를 제거

    - **변수를 결합하거나 축소**
    - 예: 평균, 비율, PCA 등의 기법 사용

    - **정규화 회귀 사용**
    - **Ridge 회귀**: L2 정규화로 계수를 작게 만들어 다중공선성 완화
    - **ElasticNet 회귀**: L1 + L2 혼합 → 변수 선택과 안정성 동시 확보

    ---

    ## 📝 한 줄 요약

    > 다중공선성은 입력 변수 간에 중복된 정보가 많아  
    > 회귀 모델이 **불안정하고 해석하기 어려워지는 현상**이다.  
    > 정규화 회귀나 변수 선택으로 해결할 수 있다.



---

오늘은 머신러닝 지도학습 중에서도 회귀에 대해 알아보았다. 

개념들이 머릿속에 뒤죽박죽 섞여 있는 듯한 느낌이 들어 구조화가 잘 된 서브노트를 만들듯이 작성해보았다.
누가 보더라도 쉽게 이해할 수 있게끔 작성하고 싶었으나 부족한 점이 많았다. 

개선 사항으로는 코드 예제를 함께 적어주거나 주석을 많이 달아주면 개념들이 한눈에 들어올 듯하다. 
또 생소할 수 있는 단어들에 대한 간단한 정의들을 작성해주어도 좋을 것 같다.

따라서 내일은 더 구체적으로 접근해서.. 회귀 모델을 사용한 코드 예제를 살펴보고, 이해한 내용에 대한 주석을 무진장 달아줄 예정!🤪

(사실 아쉬워서 commit을 못 누르고 있었는데 이젠 정말 보내줄게...💥)